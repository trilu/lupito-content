# Production Deployment Plan - AKC Breed Scraper

## Current Status âœ…

### What Works (KEEP)
- **Local scraper**: `jobs/akc_breed_scraper.py` - 100% success rate on 160 breeds
- **Flask API server**: `server.py` - Job management, file serving, monitoring endpoints
- **Monitoring**: `monitor_job_detailed.sh` - Real-time progress tracking
- **QA reporting**: Generates validation reports with extraction success metrics

### What Failed (ARCHIVED)
- **Selenium + Chrome in Cloud Run**: DevToolsActivePort errors, sandboxing conflicts
- **Multiple deployment attempts**: v1-v6 all failed due to Chrome compatibility issues

## Recommended Production Architecture

### Option 1: Hybrid Approach (RECOMMENDED)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Cloud Run     â”‚    â”‚   Local Runner   â”‚    â”‚   Supabase DB   â”‚
â”‚   (Flask API)   â”‚â—„â”€â”€â–ºâ”‚   (Scraper)      â”‚â—„â”€â”€â–ºâ”‚   (Storage)     â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ â€¢ Job triggers  â”‚    â”‚ â€¢ Actual scrapingâ”‚    â”‚ â€¢ Breed data    â”‚
â”‚ â€¢ Status checks â”‚    â”‚ â€¢ BeautifulSoup  â”‚    â”‚ â€¢ Images        â”‚
â”‚ â€¢ File serving  â”‚    â”‚ â€¢ Local executionâ”‚    â”‚ â€¢ Metadata      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits:**
- âœ… Reliable scraping (proven to work)
- âœ… Scalable API endpoints
- âœ… Cost-effective (no browser resources in cloud)
- âœ… Easy monitoring and management

### Option 2: Managed Service
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Cloud Run     â”‚    â”‚  ScrapingBee     â”‚    â”‚   Supabase DB   â”‚
â”‚   (Flask API)   â”‚â—„â”€â”€â–ºâ”‚  (Browser API)   â”‚    â”‚   (Storage)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits:**
- âœ… Fully cloud-based
- âœ… Managed browser infrastructure
- âŒ Additional cost (~$50-100/month)
- âŒ External dependency

### Option 3: VM Deployment
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Google Compute Engine       â”‚    â”‚   Supabase DB   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚   (Storage)     â”‚
â”‚  â”‚ Flask API   â”‚ â”‚ Local Scraper   â”‚ â”‚â—„â”€â”€â–ºâ”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benefits:**
- âœ… Full control over environment
- âœ… Can run browser if needed
- âŒ Higher cost and maintenance
- âŒ More complex deployment

## Implementation Roadmap

### Phase 1: Immediate (1-2 days)
1. âœ… **Archive failed attempts** - COMPLETED
2. âœ… **Document lessons learned** - COMPLETED
3. **Deploy working Flask API to Cloud Run**
   - Remove Chrome/Selenium dependencies
   - Keep job management endpoints
   - Deploy as lightweight service

### Phase 2: Data Integration (3-5 days)
1. **Create Supabase import script**
   ```python
   # Import extracted breed data to Supabase
   # Handle deduplication and updates
   # Generate success/failure reports
   ```

2. **Test data pipeline**
   - Run local scraper
   - Import to Supabase
   - Validate results

### Phase 3: Production Setup (1 week)
1. **Set up automated scheduling**
   - Cron job for periodic updates
   - Error notifications
   - Data freshness monitoring

2. **Create monitoring dashboard**
   - Breed count tracking
   - Data completeness metrics
   - Error reporting

## Deployment Commands

### Deploy Flask API (No Browser)
```bash
# Create lightweight Dockerfile
cat > Dockerfile << 'EOF'
FROM python:3.9-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY server.py .
COPY monitor_job.sh .
EXPOSE 8080
ENV PORT=8080
CMD ["python3", "server.py"]
EOF

# Build and deploy
gcloud builds submit --tag us-central1-docker.pkg.dev/breed-data/docker-repo/akc-api:production
gcloud run deploy akc-api \
  --image us-central1-docker.pkg.dev/breed-data/docker-repo/akc-api:production \
  --region us-central1 \
  --allow-unauthenticated \
  --memory 1Gi \
  --cpu 1
```

### Local Scraper Setup
```bash
# Install dependencies locally
pip install requests beautifulsoup4 python-dotenv supabase

# Run scraper
python3 jobs/akc_breed_scraper.py --urls-file akc_breed_urls.txt

# Import to Supabase (to be created)
python3 import_to_supabase.py --file latest_breed_data.json
```

## Cost Analysis

### Current (Failed) Approach
- **Cloud Run**: $20-50/month (high memory, CPU for Chrome)
- **Build minutes**: $10-20/month (multiple failed deployments)
- **Total**: $30-70/month + time cost

### Recommended Approach
- **Cloud Run API**: $5-15/month (lightweight service)
- **Local execution**: $0 (use existing hardware)
- **Total**: $5-15/month

## Risk Mitigation

### Data Loss Prevention
- âœ… **QA reports generated** - Track what was extracted
- âœ… **JSON file outputs** - Backup before database import
- âœ… **Git versioning** - Code and results tracked

### Monitoring
- âœ… **Real-time progress tracking** - Working monitoring scripts
- âœ… **Error detection** - Comprehensive logging
- âœ… **Success metrics** - 100% extraction rate achieved

### Scalability
- **Horizontal**: Run multiple local scrapers in parallel
- **Vertical**: Upgrade to managed services if volume increases
- **Geographic**: Deploy in multiple regions if needed

## Next Actions

1. **Immediate**: Deploy lightweight Flask API to Cloud Run
2. **Short-term**: Create Supabase import pipeline
3. **Medium-term**: Set up automated scheduling
4. **Long-term**: Consider migration to managed services if scale requires

## Files Structure (Clean)

```
lupito-content/
â”œâ”€â”€ jobs/
â”‚   â”œâ”€â”€ akc_breed_scraper.py          # âœ… WORKING LOCAL SCRAPER
â”‚   â””â”€â”€ import_to_supabase.py         # ðŸ”² TO CREATE
â”œâ”€â”€ server.py                         # âœ… Flask API (reusable)
â”œâ”€â”€ monitor_job_detailed.sh           # âœ… Monitoring script
â”œâ”€â”€ akc_breed_urls.txt                # âœ… Input URLs
â”œâ”€â”€ akc_breed_qa_report_*.csv         # âœ… QA reports
â”œâ”€â”€ AKC_SCRAPER_DOCUMENTATION.md      # âœ… Updated docs
â”œâ”€â”€ PRODUCTION_DEPLOYMENT_PLAN.md     # âœ… This file
â””â”€â”€ archive/failed_attempts/          # ðŸ—‚ï¸ Selenium/Chrome archive
    â”œâ”€â”€ README.md
    â”œâ”€â”€ Dockerfile.akc
    â”œâ”€â”€ akc_selenium_scraper.py
    â””â”€â”€ requirements.akc.optimized.txt
```

**Decision**: Proceed with **Option 1 (Hybrid Approach)** - proven, cost-effective, reliable.