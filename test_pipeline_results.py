#!/usr/bin/env python3
"""
Test the complete pipeline results after SQL execution
"""

import os
import pandas as pd
from datetime import datetime
from dotenv import load_dotenv
from supabase import create_client
import logging

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_pipeline():
    """Test all components of the pipeline"""
    
    # Load environment
    load_dotenv()
    url = os.getenv('SUPABASE_URL')
    key = os.getenv('SUPABASE_SERVICE_KEY')
    supabase = create_client(url, key)
    
    print("="*80)
    print("LUPITO CATALOG PIPELINE - TEST RESULTS")
    print("="*80)
    
    # Test each component
    tables_to_test = [
        'food_candidates_compat',
        'food_candidates_sc_compat', 
        'food_brands_compat',
        'foods_union_all',
        'foods_canonical',
        'foods_published'
    ]
    
    results = {}
    
    print("\nüìä TABLE/VIEW ROW COUNTS:")
    print("-"*50)
    
    for table in tables_to_test:
        try:
            response = supabase.table(table).select('*', count='exact').limit(0).execute()
            count = response.count
            results[table] = count
            status = "‚úÖ" if count > 0 else "‚ö†Ô∏è"
            print(f"{status} {table}: {count:,} rows")
        except Exception as e:
            print(f"‚ùå {table}: Error - {str(e)[:50]}")
            results[table] = 0
    
    # Test canonical deduplication
    if results.get('foods_union_all', 0) > 0 and results.get('foods_canonical', 0) > 0:
        duplicates_merged = results['foods_union_all'] - results['foods_canonical']
        print(f"\nüîÑ Duplicates merged: {duplicates_merged:,}")
    
    # Test coverage metrics on foods_canonical
    print("\nüìà COVERAGE METRICS (foods_canonical):")
    print("-"*50)
    
    try:
        # Get sample of canonical data for analysis
        response = supabase.table('foods_canonical').select('*').limit(1000).execute()
        
        if response.data:
            df = pd.DataFrame(response.data)
            total = len(df)
            
            # Life stage coverage
            life_stage_not_null = df['life_stage'].notna().sum()
            life_stage_specific = df['life_stage'].isin(['puppy', 'adult', 'senior', 'all']).sum()
            
            # Nutrition coverage
            kcal_not_null = df['kcal_per_100g_final'].notna().sum()
            
            # Ingredients coverage
            ingredients_not_empty = 0
            for tokens in df.get('ingredients_tokens', []):
                if tokens and isinstance(tokens, list) and len(tokens) > 0:
                    ingredients_not_empty += 1
            
            # Price coverage
            price_not_null = df['price_per_kg'].notna().sum()
            
            print(f"Life stage populated: {life_stage_not_null}/{total} ({life_stage_not_null/total*100:.1f}%)")
            print(f"Life stage specific: {life_stage_specific}/{total} ({life_stage_specific/total*100:.1f}%)")
            print(f"Kcal data available: {kcal_not_null}/{total} ({kcal_not_null/total*100:.1f}%)")
            print(f"Ingredients tokens: {ingredients_not_empty}/{total} ({ingredients_not_empty/total*100:.1f}%)")
            print(f"Price data: {price_not_null}/{total} ({price_not_null/total*100:.1f}%)")
            
    except Exception as e:
        print(f"‚ùå Could not analyze coverage: {str(e)[:100]}")
    
    # Test foods_published view
    print("\nüéØ FOODS_PUBLISHED VIEW (AI-ready):")
    print("-"*50)
    
    try:
        # Get sample from published view
        response = supabase.table('foods_published').select('*').limit(5).execute()
        
        if response.data:
            print(f"‚úÖ View accessible with {results.get('foods_published', 0):,} rows")
            print("\nSample records:")
            
            for i, row in enumerate(response.data[:3], 1):
                print(f"\n{i}. {row.get('brand', 'N/A')} - {row.get('product_name', 'N/A')[:40]}")
                print(f"   Form: {row.get('form', 'N/A')}, Life Stage: {row.get('life_stage', 'N/A')}")
                print(f"   Kcal: {row.get('kcal_per_100g', 'N/A')}, Primary Protein: {row.get('primary_protein', 'N/A')}")
                print(f"   Has Chicken: {row.get('has_chicken', 'N/A')}, Price Bucket: {row.get('price_bucket', 'N/A')}")
                
    except Exception as e:
        print(f"‚ùå Error accessing foods_published: {str(e)[:100]}")
    
    # Check indexes
    print("\nüîç INDEX VERIFICATION:")
    print("-"*50)
    
    try:
        # Query to check indexes (this would need direct SQL access)
        print("‚úÖ Indexes should be created:")
        print("  - idx_foods_canonical_product_key (UNIQUE)")
        print("  - idx_foods_canonical_brand_slug")
        print("  - idx_foods_canonical_life_stage")
        print("  - idx_foods_canonical_form")
        print("  - idx_foods_canonical_countries_gin (GIN)")
        print("  - idx_foods_canonical_tokens_gin (GIN)")
    except:
        pass
    
    # Summary
    print("\n" + "="*80)
    print("SUMMARY")
    print("="*80)
    
    all_created = all(v > 0 for v in results.values() if v is not None)
    
    if all_created:
        print("‚úÖ All pipeline components successfully created!")
        print(f"‚úÖ Total unique products in catalog: {results.get('foods_canonical', 0):,}")
        print("‚úÖ AI can now use foods_published view")
    else:
        print("‚ö†Ô∏è Some components may need attention")
        failed = [k for k, v in results.items() if v == 0]
        if failed:
            print(f"   Missing: {', '.join(failed)}")
    
    print("\nüìå Next steps:")
    print("  1. Configure AI to use CATALOG_VIEW_NAME=foods_published")
    print("  2. Monitor data quality metrics")
    print("  3. Set up regular refresh schedule")
    print("="*80)
    
    return results

if __name__ == "__main__":
    test_pipeline()